[Universal approximation theorem - Wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
The idea of UAT is that with enough neurons, a NeuralNetwork can represent any arbitrary function. (Just existence proof, and says nothing about training data.)

Need non-linear [[ActivationFunction]], like ReLU.

## See also
[[VideosPrematureAbstraction#[Can you really use ANY activation function? (Universal Approximation Theorem) - YouTube](https //www.youtube.com/watch?v=IonQTF7kCMg)]]

[Why Neural Networks can learn (almost) anything - Emergent Garden](https://www.youtube.com/watch?v=0QczhVg5HaI) 


