AKA System I and System II

After reading Daniel Kahneman's *Thinking Fast and Slow* one idea I came to on 2024-06-02:
>AIs are like System One thinking, and are often wrong but come up with any associative facts

- Was thinking, could you repeatedly trigger System One thinking, and "simulate" System Two thinking step-by-step?
- This made me thing of [[ExpLang]] with associative lookup
- Now it's very interesting to see OpenAI o1, with Chain of Though

## System 1 vs. System 2 explained
The name are terrible (like Type 1 vs Type 2 errors in medical tests...)
https://thedecisionlab.com/reference-guide/philosophy/system-1-and-system-2-thinking
>**System 1** is fast, automatic, and intuitive, operating with little to no effort. This mode of thinking allows us to make quick decisions and judgments based on patterns and experiences.
>**System 2** is slow, deliberate, and conscious, requiring intentional effort. This type of thinking is used for complex problem-solving and analytical tasks where more thought and consideration are necessary.

Also, Veritasium's [The Science of Thinking](https://www.youtube.com/watch?v=UBVV8pch1dM) is a great video explainer

## HN thread on "Large Language Models Think Too Fast To Explore Effectively"
From [Maps well to Kahneman's "Thinking Fast and Slow" framework system 1 thinking for... | Hacker News](https://news.ycombinator.com/item?id=42890835)
> system 1 thinking for early layer processing of uncertainty in LLMs. quick, intuitive decisions, focuses on uncertainty, happens in early transformer layers.
> system 2 thinking for later layer processing of empowerment (selecting elements that maximize future possibilities). strategic, deliberate evaluation, considering long-term possibilities, happens in later layers.
> 
> system 1 = 4o/llama 3.1
> system 1 + system 2 = o1/r1 reasoning models